{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EATc1VSjZ_2g"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import pickle\n",
    "!pip install openai\n",
    "import openai\n",
    "\n",
    "from openai.embeddings_utils import (\n",
    "    get_embedding,\n",
    "    distances_from_embeddings,\n",
    "    tsne_components_from_embeddings,\n",
    "    chart_from_components,\n",
    "    indices_of_nearest_neighbors_from_distances,\n",
    ")\n",
    "\n",
    "# constants\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "openai.api_key = '' #Insert your openAi token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPvDPcR6Z_2h"
   },
   "source": [
    "### 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9f4-krikZ_2h",
    "outputId": "2e47e23a-1766-4f21-80b9-ad08def58fd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'genbench_ds'...\n",
      "remote: Enumerating objects: 330, done.\u001b[K\n",
      "remote: Counting objects: 100% (330/330), done.\u001b[K\n",
      "remote: Compressing objects: 100% (198/198), done.\u001b[K\n",
      "remote: Total 330 (delta 133), reused 273 (delta 96), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (330/330), 8.49 MiB | 8.73 MiB/s, done.\n",
      "Resolving deltas: 100% (133/133), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/drndr/genbench_ds.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qxZ_PTFCvO0I"
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g__puHQI37i_",
    "outputId": "03d9a117-6d90-473e-fdc4-d9314c69fb00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data files:   0% 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0% 0.00/488M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   0% 1.78M/488M [00:00<00:27, 17.8MB/s]\u001b[A\n",
      "Downloading data:   2% 7.39M/488M [00:00<00:11, 40.3MB/s]\u001b[A\n",
      "Downloading data:   2% 12.0M/488M [00:00<00:11, 43.0MB/s]\u001b[A\n",
      "Downloading data:   4% 18.0M/488M [00:00<00:09, 49.6MB/s]\u001b[A\n",
      "Downloading data:   5% 25.3M/488M [00:00<00:07, 58.1MB/s]\u001b[A\n",
      "Downloading data:   7% 32.8M/488M [00:00<00:07, 63.7MB/s]\u001b[A\n",
      "Downloading data:   8% 39.1M/488M [00:00<00:07, 62.5MB/s]\u001b[A\n",
      "Downloading data:   9% 45.4M/488M [00:00<00:07, 60.6MB/s]\u001b[A\n",
      "Downloading data:  11% 52.0M/488M [00:00<00:06, 62.3MB/s]\u001b[A\n",
      "Downloading data:  12% 58.4M/488M [00:01<00:06, 62.6MB/s]\u001b[A\n",
      "Downloading data:  13% 64.6M/488M [00:01<00:07, 59.8MB/s]\u001b[A\n",
      "Downloading data:  15% 71.9M/488M [00:01<00:06, 63.4MB/s]\u001b[A\n",
      "Downloading data:  16% 78.2M/488M [00:01<00:06, 61.3MB/s]\u001b[A\n",
      "Downloading data:  18% 85.8M/488M [00:01<00:06, 65.3MB/s]\u001b[A\n",
      "Downloading data:  19% 92.9M/488M [00:01<00:05, 67.1MB/s]\u001b[A\n",
      "Downloading data:  20% 99.7M/488M [00:01<00:05, 67.3MB/s]\u001b[A\n",
      "Downloading data:  22% 106M/488M [00:01<00:05, 67.2MB/s] \u001b[A\n",
      "Downloading data:  23% 113M/488M [00:01<00:05, 67.9MB/s]\u001b[A\n",
      "Downloading data:  25% 120M/488M [00:01<00:05, 68.3MB/s]\u001b[A\n",
      "Downloading data:  26% 127M/488M [00:02<00:06, 58.8MB/s]\u001b[A\n",
      "Downloading data:  28% 134M/488M [00:02<00:05, 62.5MB/s]\u001b[A\n",
      "Downloading data:  29% 141M/488M [00:02<00:05, 63.8MB/s]\u001b[A\n",
      "Downloading data:  30% 148M/488M [00:02<00:05, 65.1MB/s]\u001b[A\n",
      "Downloading data:  32% 155M/488M [00:02<00:05, 66.0MB/s]\u001b[A\n",
      "Downloading data:  33% 161M/488M [00:02<00:04, 65.6MB/s]\u001b[A\n",
      "Downloading data:  35% 169M/488M [00:02<00:04, 68.0MB/s]\u001b[A\n",
      "Downloading data:  36% 176M/488M [00:02<00:04, 69.6MB/s]\u001b[A\n",
      "Downloading data:  38% 183M/488M [00:02<00:04, 67.4MB/s]\u001b[A\n",
      "Downloading data:  39% 190M/488M [00:03<00:04, 67.7MB/s]\u001b[A\n",
      "Downloading data:  40% 197M/488M [00:03<00:04, 68.6MB/s]\u001b[A\n",
      "Downloading data:  42% 204M/488M [00:03<00:04, 68.9MB/s]\u001b[A\n",
      "Downloading data:  43% 211M/488M [00:03<00:04, 68.8MB/s]\u001b[A\n",
      "Downloading data:  45% 218M/488M [00:03<00:03, 69.5MB/s]\u001b[A\n",
      "Downloading data:  46% 225M/488M [00:03<00:04, 65.1MB/s]\u001b[A\n",
      "Downloading data:  48% 232M/488M [00:03<00:04, 56.2MB/s]\u001b[A\n",
      "Downloading data:  49% 237M/488M [00:03<00:04, 50.7MB/s]\u001b[A\n",
      "Downloading data:  50% 243M/488M [00:03<00:05, 47.8MB/s]\u001b[A\n",
      "Downloading data:  51% 248M/488M [00:04<00:05, 40.2MB/s]\u001b[A\n",
      "Downloading data:  52% 252M/488M [00:04<00:06, 35.5MB/s]\u001b[A\n",
      "Downloading data:  53% 258M/488M [00:04<00:05, 40.0MB/s]\u001b[A\n",
      "Downloading data:  54% 264M/488M [00:04<00:04, 46.6MB/s]\u001b[A\n",
      "Downloading data:  56% 271M/488M [00:04<00:04, 52.1MB/s]\u001b[A\n",
      "Downloading data:  57% 277M/488M [00:04<00:03, 53.9MB/s]\u001b[A\n",
      "Downloading data:  58% 284M/488M [00:04<00:03, 57.8MB/s]\u001b[A\n",
      "Downloading data:  60% 291M/488M [00:04<00:03, 61.7MB/s]\u001b[A\n",
      "Downloading data:  61% 298M/488M [00:05<00:02, 64.4MB/s]\u001b[A\n",
      "Downloading data:  62% 305M/488M [00:05<00:02, 63.6MB/s]\u001b[A\n",
      "Downloading data:  64% 311M/488M [00:05<00:02, 62.9MB/s]\u001b[A\n",
      "Downloading data:  65% 318M/488M [00:05<00:02, 64.2MB/s]\u001b[A\n",
      "Downloading data:  67% 325M/488M [00:05<00:02, 65.7MB/s]\u001b[A\n",
      "Downloading data:  68% 331M/488M [00:05<00:02, 63.7MB/s]\u001b[A\n",
      "Downloading data:  69% 338M/488M [00:05<00:02, 56.8MB/s]\u001b[A\n",
      "Downloading data:  70% 344M/488M [00:05<00:02, 52.8MB/s]\u001b[A\n",
      "Downloading data:  72% 349M/488M [00:05<00:02, 49.2MB/s]\u001b[A\n",
      "Downloading data:  73% 354M/488M [00:06<00:02, 47.6MB/s]\u001b[A\n",
      "Downloading data:  74% 359M/488M [00:06<00:02, 46.7MB/s]\u001b[A\n",
      "Downloading data:  75% 364M/488M [00:06<00:02, 45.3MB/s]\u001b[A\n",
      "Downloading data:  76% 368M/488M [00:06<00:02, 44.9MB/s]\u001b[A\n",
      "Downloading data:  76% 373M/488M [00:06<00:02, 44.7MB/s]\u001b[A\n",
      "Downloading data:  77% 377M/488M [00:06<00:02, 44.4MB/s]\u001b[A\n",
      "Downloading data:  78% 382M/488M [00:06<00:02, 42.8MB/s]\u001b[A\n",
      "Downloading data:  79% 386M/488M [00:06<00:02, 41.9MB/s]\u001b[A\n",
      "Downloading data:  80% 390M/488M [00:06<00:02, 42.3MB/s]\u001b[A\n",
      "Downloading data:  81% 394M/488M [00:07<00:02, 42.1MB/s]\u001b[A\n",
      "Downloading data:  82% 399M/488M [00:07<00:02, 41.4MB/s]\u001b[A\n",
      "Downloading data:  83% 403M/488M [00:07<00:02, 40.8MB/s]\u001b[A\n",
      "Downloading data:  83% 407M/488M [00:07<00:02, 40.3MB/s]\u001b[A\n",
      "Downloading data:  84% 411M/488M [00:07<00:01, 40.5MB/s]\u001b[A\n",
      "Downloading data:  85% 415M/488M [00:07<00:01, 41.0MB/s]\u001b[A\n",
      "Downloading data:  86% 419M/488M [00:07<00:01, 41.3MB/s]\u001b[A\n",
      "Downloading data:  87% 424M/488M [00:07<00:01, 41.4MB/s]\u001b[A\n",
      "Downloading data:  88% 428M/488M [00:08<00:02, 24.6MB/s]\u001b[A\n",
      "Downloading data:  89% 431M/488M [00:08<00:02, 27.3MB/s]\u001b[A\n",
      "Downloading data:  89% 436M/488M [00:08<00:01, 30.2MB/s]\u001b[A\n",
      "Downloading data:  90% 439M/488M [00:08<00:01, 32.4MB/s]\u001b[A\n",
      "Downloading data:  91% 443M/488M [00:08<00:01, 34.4MB/s]\u001b[A\n",
      "Downloading data:  92% 448M/488M [00:08<00:01, 36.4MB/s]\u001b[A\n",
      "Downloading data:  93% 452M/488M [00:08<00:00, 36.9MB/s]\u001b[A\n",
      "Downloading data:  93% 455M/488M [00:08<00:00, 37.5MB/s]\u001b[A\n",
      "Downloading data:  94% 460M/488M [00:08<00:00, 38.5MB/s]\u001b[A\n",
      "Downloading data:  95% 464M/488M [00:08<00:00, 39.5MB/s]\u001b[A\n",
      "Downloading data:  96% 468M/488M [00:09<00:00, 40.2MB/s]\u001b[A\n",
      "Downloading data:  97% 472M/488M [00:09<00:00, 40.3MB/s]\u001b[A\n",
      "Downloading data:  98% 476M/488M [00:09<00:00, 40.2MB/s]\u001b[A\n",
      "Downloading data:  99% 480M/488M [00:09<00:00, 40.8MB/s]\u001b[A\n",
      "Downloading data: 100% 488M/488M [00:09<00:00, 51.1MB/s]\n",
      "Downloading data files: 100% 1/1 [00:10<00:00, 10.25s/it]\n",
      "Extracting data files: 100% 1/1 [00:18<00:00, 18.19s/it]\n",
      "Extracting data files: 100% 3/3 [00:09<00:00,  3.23s/it]\n",
      "Generating train split:  32% 101074/317832 [00:58<01:56, 1867.84 examples/s]"
     ]
    }
   ],
   "source": [
    "!python /content/genbench_ds/data/mrr/codesearchnet_go/create_ds_mrr_go.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4Z0pkveZ_2i"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_jsonl(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "filename = '/content/test_go_mrr.jsonl'\n",
    "data = read_jsonl(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHxI44vQsRgZ"
   },
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YPH7ixrvn1O1"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('test_go.json', 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "P27oxCExTBO7"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Dict, List\n",
    "import tqdm\n",
    "import datasets\n",
    "import numpy as np\n",
    "from more_itertools import chunked\n",
    "\n",
    "\n",
    "def get_dataset_raw(n_distractors) -> Dict[str, datasets.Dataset]:\n",
    "    raw_datasets: Dict[str, datasets.Dataset] = _load_data_source()\n",
    "    output: Dict[str, datasets.Dataset] = {}\n",
    "    random.seed(42)\n",
    "\n",
    "    for split, dataset in raw_datasets.items():\n",
    "        if split == \"test\":\n",
    "            # Convert dataset to list for easier manipulation\n",
    "            dataset_list = list(dataset)\n",
    "\n",
    "            new_data = []\n",
    "\n",
    "            for idx, item in enumerate(dataset_list):\n",
    "                new_data.append(item)\n",
    "\n",
    "                # Create other_items list once and then simply exclude the current item during sampling\n",
    "                other_items = dataset_list[:idx] + dataset_list[idx+1:]\n",
    "                random_items = random.sample(other_items, n_distractors)\n",
    "\n",
    "                input_parts = item[\"input\"].split(\"[CODESPLIT]\")\n",
    "\n",
    "                for random_item in random_items:\n",
    "                    random_input_parts = random_item[\"input\"].split(\"[CODESPLIT]\")\n",
    "                    new_input = input_parts[0] + \"[CODESPLIT]\" + random_input_parts[1]\n",
    "                    new_item = {\"input\": new_input, \"target\": 0, \"target_options\": item[\"target_options\"]}\n",
    "                    new_data.append(new_item)\n",
    "                print(len(new_data))\n",
    "\n",
    "            # Convert list back to HuggingFace dataset\n",
    "            output[split] = datasets.Dataset.from_dict({k: [dic[k] for dic in new_data] for k in new_data[0]})\n",
    "        else:\n",
    "            output[split] = dataset\n",
    "\n",
    "    return output\n",
    "\n",
    "def _load_data_source() -> Dict[str, datasets.Dataset]:\n",
    "    return datasets.load_dataset(\"json\", data_files={'test':'/content/test_go.json'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fYx9ln5JcpGX"
   },
   "outputs": [],
   "source": [
    "n_distractors = 99  # adjust this number as per your requirements\n",
    "new_datasets = get_dataset_raw(n_distractors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cm8ryMPX4Hj_",
    "outputId": "fd0be73c-d99c-40d7-ad92-b61b1ba1653e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227900"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_datasets['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjeQNDF-Z_2i"
   },
   "source": [
    "### 3. Build cache to save embeddings\n",
    "\n",
    "Before getting embeddings for these articles, let's set up a cache to save the embeddings we generate. In general, it's a good idea to save your embeddings so you can re-use them later. If you don't save them, you'll pay again each time you compute them again.\n",
    "\n",
    "The cache is a dictionary that maps tuples of `(text, model)` to an embedding, which is a list of floats. The cache is saved as a Python pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4fTTLYYhZ_2i"
   },
   "outputs": [],
   "source": [
    "# establish a cache of embeddings to avoid recomputing\n",
    "# cache is a dict of tuples (text, model) -> embedding, saved as a pickle file\n",
    "\n",
    "# set path to embedding cache\n",
    "embedding_cache_path = \"recommendations_embeddings_cache_go.pkl\"\n",
    "\n",
    "# load the cache if it exists, and save a copy to disk\n",
    "try:\n",
    "    embedding_cache = pd.read_pickle(embedding_cache_path)\n",
    "except FileNotFoundError:\n",
    "    embedding_cache = {}\n",
    "with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n",
    "    pickle.dump(embedding_cache, embedding_cache_file)\n",
    "\n",
    "# define a function to retrieve embeddings from the cache if present, and otherwise request via the API\n",
    "def embedding_from_string(\n",
    "    string: str,\n",
    "    model: str = EMBEDDING_MODEL,\n",
    "    embedding_cache=embedding_cache\n",
    ") -> list:\n",
    "    \"\"\"Return embedding of given string, using a cache to avoid recomputing.\"\"\"\n",
    "    if (string, model) not in embedding_cache.keys():\n",
    "        embedding_cache[(string, model)] = get_embedding(string, model)\n",
    "        with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n",
    "            pickle.dump(embedding_cache, embedding_cache_file)\n",
    "    return embedding_cache[(string, model)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oL6ptD_jZ_2i"
   },
   "source": [
    "Let's check create a dataframe to save the similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "aw02q_HV7y08",
    "outputId": "c5acffbf-a540-434a-9805-a1f3dadd2792"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-48fc944d-e9e0-4c5f-a6c9-5adb3413c4ac\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Prediciton</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227895</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227896</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227897</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227898</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227899</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>227900 rows Ã— 2 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-48fc944d-e9e0-4c5f-a6c9-5adb3413c4ac')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-48fc944d-e9e0-4c5f-a6c9-5adb3413c4ac button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-48fc944d-e9e0-4c5f-a6c9-5adb3413c4ac');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-fc829297-2bbf-4200-9575-e9f0bbd5cf81\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fc829297-2bbf-4200-9575-e9f0bbd5cf81')\"\n",
       "            title=\"Suggest charts.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "    background-color: #E8F0FE;\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: #1967D2;\n",
       "    height: 32px;\n",
       "    padding: 0 0 0 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: #E2EBFA;\n",
       "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: #174EA6;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "    background-color: #3B4455;\n",
       "    fill: #D2E3FC;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart:hover {\n",
       "    background-color: #434B5C;\n",
       "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "    fill: #FFFFFF;\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const charts = await google.colab.kernel.invokeFunction(\n",
       "          'suggestCharts', [key], {});\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-fc829297-2bbf-4200-9575-e9f0bbd5cf81 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "        Target  Prediciton\n",
       "0            1         NaN\n",
       "1            0         NaN\n",
       "2            0         NaN\n",
       "3            0         NaN\n",
       "4            0         NaN\n",
       "...        ...         ...\n",
       "227895       0         NaN\n",
       "227896       0         NaN\n",
       "227897       0         NaN\n",
       "227898       0         NaN\n",
       "227899       0         NaN\n",
       "\n",
       "[227900 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "target = []\n",
    "for i in range(len(new_datasets['test'])):\n",
    "  target.append(new_datasets['test'][i]['target'])\n",
    "\n",
    "\n",
    "# initialize data of lists.\n",
    "data_ = {'Target': target}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data_)\n",
    "df['Prediciton'] = np.nan\n",
    "# Print the output.\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbhjUaQm4nJe"
   },
   "source": [
    "Calculate the similarity between the query and the code using Ada's embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zwn_FMFI-Rj7"
   },
   "outputs": [],
   "source": [
    "#Go dataset\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "i = 0\n",
    "while i < len(df):\n",
    "\n",
    "        input_split = new_datasets['test'][i]['input'].split('[CODESPLIT]')\n",
    "\n",
    "        x = embedding_from_string(input_split[0])\n",
    "        y = embedding_from_string(input_split[1])\n",
    "\n",
    "        df.loc[i, 'Prediciton'] = cosine_similarity([x], [y])[0][0]\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            print(f\"Progress: {i}/{len(df)}\")\n",
    "            df.to_csv('/content/drive/MyDrive/CodeInspector/Workshop dataset paper/Results/MRR/MRR_Go_sklearn.csv', index=False)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WfUeaGogReX8"
   },
   "outputs": [],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "JNRTZ6BTSmYs"
   },
   "outputs": [],
   "source": [
    "df.to_csv('MRR_Go_sklearn.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBcIsMq-TYiw"
   },
   "source": [
    "**Evaluation**\n",
    "\n",
    "Calculate the MRR results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMLj6cO512O3"
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbMDlFbR1qB_"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Dict, List\n",
    "import datasets\n",
    "import numpy as np\n",
    "from more_itertools import chunked\n",
    "\n",
    "def evaluate_predictions(predictions: List[Dict[str, float]]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate the MRR score in chunks. One chunk consist of a true comment-code pair and n number of distractors\n",
    "        This function assumes that the predictions were made and passed onto this function unshuffled.\n",
    "        The test data is ordered with each true pair followed by n number of distractors\n",
    "         Args:\n",
    "             predictions: A list of dictionaries, where each dictionary contains the predicted values for an example.\n",
    "                          The keys are strings and the values are floats (logit scores or similarity values).\n",
    "             gold: A HuggingFace `datasets.Dataset` object containing the ground truth data for the task.\n",
    "             n_distractors:  Number of distractor comment-code pair for each true pair.\n",
    "                             Must be the same number as in the get_dataset_raw function\n",
    "\n",
    "         Returns:\n",
    "             A dictionary containing key-value pairs for the evaluation metric(s) computed on the predicted\n",
    "             values. The keys are strings representing the name of the evaluation metric and the values are\n",
    "             floating-point numbers.\n",
    "        \"\"\"\n",
    "        ranks = []\n",
    "\n",
    "        batched_predictions = chunked(predictions, 99 + 1)\n",
    "\n",
    "        for batch_idx, predictions in enumerate(batched_predictions):\n",
    "            correct_score = predictions[0][\"score\"]\n",
    "            scores = np.array([prediction[\"score\"] for prediction in predictions])\n",
    "            rank = np.sum(scores >= correct_score)\n",
    "            ranks.append(rank)\n",
    "            if rank == 0:\n",
    "                print(f\"Batch {batch_idx} has a problem. Correct score: {correct_score}. All scores: {scores}\")\n",
    "            ranks.append(rank)\n",
    "        mean_mrr = np.mean(1.0 / np.array(ranks))\n",
    "\n",
    "        return {\"mean mrr\": mean_mrr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gp39o1BT1k2d"
   },
   "outputs": [],
   "source": [
    "# Convert your DataFrame's prediction column into the format the function expects\n",
    "predictions_list = [{\"score\": score} for score in df['Prediciton'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tPx_9-RP1meR",
    "outputId": "dd7e3f88-043a-43dc-d755-f35189a7f793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean mrr': 0.8942652125683934}\n"
     ]
    }
   ],
   "source": [
    "# Call the function\n",
    "n_distractors = 99  # replace this with your actual number of distractors\n",
    "result = evaluate_predictions(predictions_list)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
